---
title: "Bootstrap analysis: determine which models are better than baseline"
author: "Verena Chung, Robert Allaway (Sage Bionetworks)"
date: "`r Sys.Date()`"
output:
  html_notebook:
    df_print: paged
    code_fold: hide
    toc: true
    toc_float: true
---

## Introduction

This is the same analysis as `determine-top-performers.Rmd`, but the `refPredIndex` for calculating the Bayes factor is set based on the baseline model. This will allow you to determine which models submitted were better than the baseline provided by the organizers. 

## Setup

First, import packages for data manipulation and retrieve prediction data, gold standard data, and the template for use in the scoring code. Don't forget to set a seed!

```{r echo=TRUE, message=FALSE, warning=FALSE}
set.seed(98109)

library(tidyverse)
library(reticulate)
library(ggplot2)

# Synapse setup to use `reticulate`
use_condaenv("synapse")
synapseclient <- reticulate::import('synapseclient')
syn <- synapseclient$Synapse()
```

```{r include=FALSE}
#login
syn$login()

```

#### Functions 

Additionally, define functions to help with the scoring algorithms.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# 
calculate_weight <- function(x){
  weight <- dplyr::if_else(x == 0, 1,
            dplyr::if_else(x == 1, 2,
            dplyr::if_else(x>=2 & x<=3, 2.143547,
            dplyr::if_else(x>=4 & x<=7, 3.863745,
            dplyr::if_else(x>=8 & x<=20, 8,
            dplyr::if_else(x>=21 & x<=55, 16,
            dplyr::if_else(x>=56 & x<=148, 32,
            dplyr::if_else(x>=148, 64, 0))))))))
  return(weight)
}

# 
sum_weighted_error <- function(gold, pred, weight){
  sum(weight*abs(gold - pred))
}

# 
rmse <- function(gold, pred){
  sqrt(mean((gold - pred) ** 2))
}

# Scoring function for joint weighted sum RMSE.
score_weighted_rmse <- function(df, pred) {
  df$pred_score <- pred
  df %>% group_by(Patient_ID, weight) %>%
    summarize(patient_rmse = rmse(log2(score+1), log2(pred_score+1))) %>% 
    ungroup() %>% 
    summarize(result= sum(weight*patient_rmse)/sum(weight)) %>%
    pluck("result")
}
```

#### Goldstandard

The assessment of subchallenge 1 utilizies known SvH scores and subchallenges 2 & 3 known individual joint narrowing and erosion scores. After adding a weight for each joint score, split the goldstandard accordingly. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
gold <- read.csv(syn$get("syn22296988")$path) %>%
  mutate(weight = calculate_weight(Overall_Tol))

gold.sc1 <- gold %>% 
  select(Patient_ID, Overall_Tol, weight)

gold_joints <- gold %>%
  select(-Overall_Tol) %>%
  gather('joint', 'score', -Patient_ID, -weight)
```

## Bootstrap Submissions

Read in prediction files, combine, then bootstrap the predictions + a gold standard 1000 times to calculate 1000 scores per prediction. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
N <- 1000  # number of bootstrapped scores to be calculated

query <- syn$tableQuery(
  "SELECT id, prediction_fileid, submitterid, createdOn,
    sc1_weighted_sum_error AS sc1, 
    sc2_joint_weighted_sum_rmse AS sc2, 
    sc3_joint_weighted_sum_rmse AS sc3
  FROM syn22236264 WHERE status = 'ACCEPTED' AND 
    submitterid <> 3408914")$asDataFrame() %>%
  group_by(submitterid) %>%
  slice(which.max(createdOn)) %>%
  select(-createdOn)

# For easier identification, replace each team/participant's submitterid with
# their team name/username.
query$submitterid <- as.character(query$submitterid)
team_names <- sapply(query$submitterid, function(sub) {
  name <- tryCatch({
    syn$getUserProfile(sub)$userName
  }, error = function(err) {
    syn$getTeam(sub)$name
  })
  return(name)
})
query$submitterid <- team_names

pred_filenames <- lapply(query$prediction_fileid, function(id) {
  syn$get(id)$path
})
names(pred_filenames) <- team_names

# Before bootstrapping, rearrange the teams by rank, where left = better 
# performance and right = not-as-great performance.

### SC1
submissions.sc1 <- lapply(names(pred_filenames), function(team) {
  read.csv(pred_filenames[[team]]) %>% 
    mutate_at(vars(-Patient_ID), ~ replace(., which(.<0), 0)) %>%
    select(Patient_ID, Overall_Tol) %>%
    rename(!!team := Overall_Tol)
}) %>%
  reduce(left_join, by="Patient_ID") %>%
  left_join(gold.sc1, by="Patient_ID") %>%
  rename(gold = Overall_Tol)

results.sc1 <- submissions.sc1[,c("gold", "weight", (query %>% arrange(sc1))$submitterid)]

bs_indices.sc1 <- matrix(1:nrow(results.sc1), nrow(results.sc1), N) %>%
  apply(2, sample, replace = T)

boot.sc1 <- apply(bs_indices.sc1, 2, function(ind) {
  tmp.gold <- results.sc1[ind,c(1:2)]
  apply(results.sc1[ind, -c(1:2)], 2, function(pred) {
    sum_weighted_error(
      log2(tmp.gold$gold + 1),
      log2(pred + 1),
      tmp.gold$weight
    ) / sum(tmp.gold$weight)
  })
}) %>%
  t()

### SC2
submissions.sc23 <- lapply(names(pred_filenames), function(team) {
  read.csv(pred_filenames[[team]]) %>% 
    select(-Overall_Tol) %>%
    gather('joint', !!team, -Patient_ID)
}) %>%
  reduce(left_join, by=c("Patient_ID", "joint")) %>%
  left_join(gold_joints, by=c("Patient_ID", "joint"))

results.sc2 <- submissions.sc23[, c(
    "Patient_ID", "joint", "weight", "score", (query %>% arrange(sc2))$submitterid
  )] %>%
  filter(grepl(".+_J__.+", joint))

bs_indices.sc2 <- matrix(1:nrow(results.sc2), nrow(results.sc2), N) %>%
  apply(2, sample, replace = T)

boot.sc2 <- apply(bs_indices.sc2, 2, function(ind) {
  tmp.gold <- results.sc2[ind, c(1:4)]
  apply(results.sc2[ind, -c(1:4)], 2, function(pred) {
    score_weighted_rmse(tmp.gold, pred)
  })
}) %>%
  t()

### SC3
results.sc3 <- submissions.sc23[, c(
    "Patient_ID", "joint", "weight", "score", (query %>% arrange(sc3))$submitterid
  )] %>%
  filter(grepl(".+_E__.+", joint))

bs_indices.sc3 <- matrix(1:nrow(results.sc3), nrow(results.sc3), N) %>%
  apply(2, sample, replace = T)

boot.sc3 <- apply(bs_indices.sc3, 2, function(ind) {
  tmp.gold <- results.sc3[ind, c(1:4)]
  apply(results.sc3[ind, -c(1:4)], 2, function(pred) {
    score_weighted_rmse(tmp.gold, pred)
  })
}) %>%
  t()
```

## Bayes Factor Analysis

The Bayes factor calculation for the analysis against the baseline model is a little bit more complicated because the reference is in the middle.  This consequently means that the inversion of K (BF) will be dependent on which submission is being compared to the baseline.

```{r echo=TRUE, message=FALSE, warning=FALSE}
computeBayesFactorWhereRefIsNotBest <- function(bootstrapMetricMatrix,
                               refPredIndex,
                               invertBayes){

    M <- as.data.frame(bootstrapMetricMatrix - bootstrapMetricMatrix[,refPredIndex])
    K <- apply(M ,2, function(x) {
      k <- sum(x >= 0)/sum(x < 0)
      if(sum(x >= 0) > sum(x < 0)){
      return(k)
      }else{
      return(1/k)
      }
    })
    if(invertBayes == T){K <- 1/K}
    K[refPredIndex] <- 0

    return(K)
}

# Baseline: RA2 Baseline Model (columns 15, 12 and 10)
bayes_baseline.sc1 <- computeBayesFactorWhereRefIsNotBest(boot.sc1, refPredIndex=15, invertBayes=F) %>%
  as_tibble(rownames = "submission") %>%
  rename(bayes = value)

bayes_baseline.sc2 <- computeBayesFactorWhereRefIsNotBest(boot.sc2, refPredIndex=12, invertBayes=F) %>%
  as_tibble(rownames = "submission") %>%
  rename(bayes = value)

bayes_baseline.sc3 <- computeBayesFactorWhereRefIsNotBest(boot.sc3, refPredIndex=10, invertBayes=F) %>%
  as_tibble(rownames = "submission") %>%
  rename(bayes = value)
```

#### Subchallenge 1
```{r echo=FALSE}
bayes_baseline.sc1
```

#### Subchallenge 2
```{r echo=FALSE}
bayes_baseline.sc2
```

#### Subchallenge 3
```{r echo=FALSE}
bayes_baseline.sc3
```


## Plot Results

Plot boxplot of all scores, coloring the boxes by Bayes factor. 

```{r echo=TRUE, message=FALSE, fig.height=8, fig.width=4}

plot_results <- function(results, bayes, subchallenge) {
  results %>%
    as_tibble() %>%
    gather(submission, bs_score) %>%
    left_join(bayes) %>%
    mutate(bayes_category=case_when(
      bayes == 0 ~ "Reference",
      bayes<=3 ~ "<3",
      bayes>=3 & bayes <5 ~ "3-5",
      bayes>=5 & bayes <10 ~ "5-10",
      bayes>=10 ~ ">10")) %>%
    ggplot(aes(
      x=fct_reorder(submission, -bs_score, .fun = mean),
      y=bs_score,
      color=bayes_category
    )) +
    geom_boxplot() +
    theme_bw() +
    scale_color_manual(values = c(
      "Reference"="#d32e36", 
      '<3' = '#cf4d6f', 
      "3-5" = "#cc7e85",
      "5-10" = '#c5afa4', 
      ">10" = "#a8a6a4"),
      name = "Bayes Factor") +
    coord_flip() +
    labs(x="Team", y=paste("Bootstrapped", subchallenge, "Score"))
}

sc1 <- plot_results(boot.sc1, bayes_baseline.sc1, "SC1")
sc2 <- plot_results(boot.sc2, bayes_baseline.sc2, "SC2")
sc3 <- plot_results(boot.sc3, bayes_baseline.sc3, "SC3")

cowplot::plot_grid(sc1, 
                   sc2, 
                   sc3, 
                   ncol = 1, labels = c("A", "B", "C"))

ggsave("../figures/supplemental_figure_comparison_to_baseline.png")
ggsave("../figures/supplemental_figure_comparison_to_baseline.svg")
ggsave("../figures/supplemental_figure_comparison_to_baseline.pdf")

```
