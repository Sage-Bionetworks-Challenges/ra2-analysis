---
title: 'Final round ensembled model scoring - spearman correlation'
author: "Robert Allaway (Sage Bionetworks)"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    df_print: paged
    code_fold: hide
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---


## Introduction

Often times, an ensemble of methods will perform better than the individual method. This known as the "wisdom of the crowds" phenomenon. An easy way to generate an ensemble prediction is to take the mean, median, or weighted average of all of the predictions. You then can score this "prediction" as you would any other prediction file to assess it's performance relative to the submissions. 

Another consideration is that the wisdom of the crowds method sometimes applies only to a certain point. That is, if you order all of the submitted predictions from high to low performance, there may be a point after which you no longer want to add a prediction to your ensemble method. A good visualization of this can be found in Supplemental Figure 8 [here](https://www.biorxiv.org/content/10.1101/2019.12.31.891812v3.supplementary-material), where you can see how the ensemble score changes after adding additional predictions. Performance peaks with an ensemble of the top four predictions, but does not improve with additions of further models. 

A final consideration here is that certain models may be better at predicting certain components of the problem. You may be able to more strategically ensemble methods if you assess the performance of the submitted models on categorical subsets of the data (e.g. for a method predicting among four cancer subtypes, some methods may be better at predicting subtype 1 vs the other methods, while other methods may be better at predicting subtype 3 than other methods). Weighting the predictions from these methods strategically may result in better ensemble performance.


First, import packages, scoring functions, and challenge data. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(reticulate)
# library(reactable)
use_condaenv("ra2dream", required = T) #conda environment with synapse >2.0 installed
synapse <- import('synapseclient')
syn <- synapse$Synapse()

```

```{r include=FALSE}
#login
syn$login()

```

Scoring helper functions.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# 
calculate_weight <- function(x){
  weight <- dplyr::if_else(x == 0, 1,
            dplyr::if_else(x == 1, 2,
            dplyr::if_else(x>=2 & x<=3, 2.143547,
            dplyr::if_else(x>=4 & x<=7, 3.863745,
            dplyr::if_else(x>=8 & x<=20, 8,
            dplyr::if_else(x>=21 & x<=55, 16,
            dplyr::if_else(x>=56 & x<=148, 32,
            dplyr::if_else(x>=148, 64, 0))))))))
  return(weight)
}
```

#### Goldstandard

The assessment of subchallenge 1 utilizies known SvH scores and subchallenges 2 & 3 known individual joint narrowing and erosion scores. After adding a weight for each joint score, split the goldstandard accordingly. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
gold <- read.csv(syn$get("syn22296988")$path) %>%
  mutate(weight = calculate_weight(Overall_Tol))

gold.sc1 <- gold %>% 
  select(Patient_ID, Overall_Tol, weight)

gold_joints <- gold %>%
  select(-Overall_Tol) %>%
  gather('joint', 'score', -Patient_ID, -weight)
```

---

## Bootstrap Submissions

Read in prediction files, combine, then bootstrap the predictions + a gold standard 1000 times to calculate 1000 scores per prediction. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
N <- 1000  # number of bootstrapped scores to be calculated

##filtered out Sha2256:6cb365.... because this was the wrong baseline model; we re-ran with the correct baseline (sha:2d72ae6...)

query <- syn$tableQuery(
  "SELECT id, prediction_fileid, submitterid, createdOn,
    sc1_weighted_sum_error AS sc1, 
    sc2_joint_weighted_sum_rmse AS sc2, 
    sc3_joint_weighted_sum_rmse AS sc3
  FROM syn22236264 WHERE status = 'ACCEPTED' AND 
    submitterid <> 3408914 AND
    submitterid <> 3399952 AND
    dockerdigest not in ('sha256:6cb365830745c5d368bcd128a3a1b750ace801da281356a5a623237408c10a3d')")$asDataFrame() %>%
  group_by(submitterid) %>%
  slice(which.max(createdOn)) %>%
  select(-createdOn)

# For easier identification, replace each team/participant's submitterid with
# their team name/username.
query$submitterid <- as.character(query$submitterid)
team_names <- sapply(query$submitterid, function(sub) {
  name <- tryCatch({
    syn$getUserProfile(sub)$userName
  }, error = function(err) {
    syn$getTeam(sub)$name
  })
  return(name)
})
query$submitterid <- team_names

pred_filenames <- lapply(query$prediction_fileid, function(id) {
  syn$get(id)$path
})
names(pred_filenames) <- team_names

# Before bootstrapping, rearrange the teams by rank, where left = better 
# performance and right = not-as-great performance.

### SC1
submissions.sc1 <- lapply(names(pred_filenames), function(team) {
  read.csv(pred_filenames[[team]]) %>% 
    mutate_at(vars(-Patient_ID), ~ replace(., which(.<0), 0)) %>%
    select(Patient_ID, Overall_Tol) %>%
    rename(!!team := Overall_Tol)
}) %>%
  reduce(left_join, by="Patient_ID") %>%
  left_join(gold.sc1, by="Patient_ID") %>%
  rename(gold = Overall_Tol)

results.sc1 <- submissions.sc1[,c("gold", "weight", (query %>% arrange(sc1))$submitterid)]

ensemble.sc1 <- sapply(3:ncol(results.sc1), function(x){
  print(colnames(results.sc1)[3:x])
  new_col <- results.sc1 %>% 
    select(3:x) %>% 
    rowMeans()
}) %>% 
  as.data.frame()

ens_names <- c(glue::glue("{colnames(results.sc1)[3]}"), glue::glue("+{colnames(results.sc1)[-1:-3]}"))
colnames(ensemble.sc1) <- ens_names

ensemble.sc1 <- ensemble.sc1 %>% 
  add_column(weight = results.sc1$weight) %>% 
  add_column(gold = results.sc1$gold)

ensemble.sc1 <- ensemble.sc1[,c("gold", "weight", ens_names)]

p<-lapply(ens_names, function(x){
  correlation <- cor(ensemble.sc1$gold, ensemble.sc1[[x]], method = 'spearman')
  ggplot(data = ensemble.sc1) +
    geom_point(aes(x = gold, y = !!sym(x))) +
    ggtitle(glue::glue("spearman = {correlation}"))
})

bs_indices.sc1 <- matrix(1:nrow(ensemble.sc1), nrow(ensemble.sc1), N) %>%
  apply(2, sample, replace = T)

boot.sc1 <- apply(bs_indices.sc1, 2, function(ind) {
  tmp.gold <- ensemble.sc1[ind,c(1:2)]
  apply(ensemble.sc1[ind, -c(1:2)], 2, function(pred) {
    cor(
      tmp.gold$gold,
      pred,
      method = "spearman"
    )
  })
}) %>%
  t()




### SC2
submissions.sc2 <- lapply(names(pred_filenames), function(team) {
  read.csv(pred_filenames[[team]]) %>% 
    select(-Overall_Tol) %>%
    gather('joint', !!team, -Patient_ID)
}) %>%
  reduce(left_join, by=c("Patient_ID", "joint")) %>%
  left_join(gold_joints, by=c("Patient_ID", "joint"))

results.sc2 <- submissions.sc2[, c(
    "Patient_ID", "joint", "weight", "score", (query %>% arrange(sc2))$submitterid
  )] %>%
  filter(grepl(".+_J__.+", joint))

ensemble.sc2 <- sapply(5:ncol(results.sc2), function(x){
  new_col <- results.sc2 %>% 
    select(5:x) %>% 
    mutate(!!sym(as.character(x)) := rowMeans(across(where(is.numeric)))) %>% 
    pluck(as.character(x))
}) %>% 
  as.data.frame()

ens_names <- c(glue::glue("{colnames(results.sc2)[5]}"), glue::glue("+{colnames(results.sc2)[-1:-5]}"))
colnames(ensemble.sc2) <- ens_names

ensemble.sc2 <- ensemble.sc2 %>% 
  add_column(weight = results.sc2$weight,
             score = results.sc2$score,
             Patient_ID = results.sc2$Patient_ID,
             joint = results.sc2$joint)

ensemble.sc2 <- ensemble.sc2[,c("Patient_ID", "joint", "weight", "score", ens_names)]

p<-lapply(ens_names, function(x){
  correlation <- cor(ensemble.sc2$score, ensemble.sc2[[x]], method = 'spearman') 
  ggplot(data = ensemble.sc2) +
    geom_point(aes(x = score, y = !!sym(x))) +
    ggtitle(glue::glue("spearman = {correlation}"))
})

p

bs_indices.sc2 <- matrix(1:nrow(ensemble.sc2), nrow(ensemble.sc2), N) %>%
  apply(2, sample, replace = T)

boot.sc2 <- apply(bs_indices.sc2, 2, function(ind) {
  tmp.gold <- ensemble.sc2[ind, c(1:4)]
  apply(ensemble.sc2[ind, -c(1:4)], 2, function(pred) {
    cor(
      tmp.gold$score,
      pred,
      method = "spearman"
    )
    })
}) %>%
  t()

### SC3
submissions.sc3 <- lapply(names(pred_filenames), function(team) {
  read.csv(pred_filenames[[team]]) %>% 
    select(-Overall_Tol) %>%
    gather('joint', !!team, -Patient_ID)
}) %>%
  reduce(left_join, by=c("Patient_ID", "joint")) %>%
  left_join(gold_joints, by=c("Patient_ID", "joint"))

results.sc3 <- submissions.sc3[, c(
    "Patient_ID", "joint", "weight", "score", (query %>% arrange(sc3))$submitterid
  )] %>%
  filter(grepl(".+_E__.+", joint))

ensemble.sc3 <- sapply(5:ncol(results.sc3), function(x){
  new_col <- results.sc3 %>% 
    select(5:x) %>% 
    mutate(!!sym(as.character(x)) := rowMeans(across(where(is.numeric)))) %>% 
    pluck(as.character(x))
}) %>% 
  as.data.frame()

ens_names <- c(glue::glue("{colnames(results.sc3)[5]}"), glue::glue("+{colnames(results.sc3)[-1:-5]}"))
colnames(ensemble.sc3) <- ens_names

ensemble.sc3 <- ensemble.sc3 %>% 
  add_column(weight = results.sc3$weight,
             score = results.sc3$score,
             Patient_ID = results.sc3$Patient_ID,
             joint = results.sc3$joint)

ensemble.sc3 <- ensemble.sc3[,c("Patient_ID", "joint", "weight", "score", ens_names)]

p<-lapply(ens_names, function(x){
  correlation <- cor(ensemble.sc3$score, ensemble.sc3[[x]], method = 'spearman') 
  ggplot(data = ensemble.sc3) +
    geom_point(aes(x = score, y = !!sym(x))) +
    ggtitle(glue::glue("spearman = {correlation}"))
})

p

bs_indices.sc3 <- matrix(1:nrow(ensemble.sc3), nrow(ensemble.sc3), N) %>%
  apply(2, sample, replace = T)

boot.sc3 <- apply(bs_indices.sc3, 2, function(ind) {
  tmp.gold <- ensemble.sc3[ind, c(1:4)]
  apply(ensemble.sc3[ind, -c(1:4)], 2, function(pred) {
    cor(
      tmp.gold$score,
      pred,
      method = "spearman"
    )
  })
}) %>%
  t()
```

---

## Bayes Factor Analysis

Use our `challengescoring` package to compute Bayes factors using a matrix of scores, setting the `refPredIndex` as the number of the column that contains the top prediction (the reference prediction).

In addition to computing BF against the top performer, we are also interested in comparing against all columns. Some tweaking to the computation will be required, as the reference index may no longer be the best, which in consequence, means the inversion of K will be dependent on which submission it is being compared with.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(challengescoring)
computeBayesFactorWhereRefIsNotBest <- function(bootstrapMetricMatrix,
                               refPredIndex,
                               invertBayes){

    M <- as.data.frame(bootstrapMetricMatrix - bootstrapMetricMatrix[,refPredIndex])
    K <- apply(M ,2, function(x) {
      k <- sum(x >= 0)/sum(x < 0)
      if(sum(x >= 0) > sum(x < 0)){
      return(k)
      }else{
      return(1/k)
      }
    })
    if(invertBayes == T){K <- 1/K}
    K[refPredIndex] <- 0

    return(K)
}

# Top performer: Team Shirin
bayes_top.sc1 <- computeBayesFactorWhereRefIsNotBest(boot.sc1, refPredIndex=1, invertBayes=F) %>%
  as_tibble(rownames = "submission") %>%
  rename(bayes = value)

# Top performer: Hongyang Li and Yuanfang Guan (column 1)
bayes_top.sc2 <- computeBayesFactorWhereRefIsNotBest(boot.sc2, refPredIndex=1, invertBayes=F) %>%
  as_tibble(rownames = "submission") %>%
  rename(bayes = value)

# Top performer: Gold Therapy (column 1)
bayes_top.sc3 <- computeBayesFactorWhereRefIsNotBest(boot.sc3, refPredIndex=1, invertBayes=F) %>%
  as_tibble(rownames = "submission") %>%
  rename(bayes = value) 


# bayesFactorMatrix <- function(bootstrapMatrix) {
#   names <- colnames(bootstrapMatrix)
#   results <- purrr::map(1:ncol(bootstrapMatrix), function(ind) {
#     computeBayesFactorWhereRefIsNotBest(bootstrapMatrix, refPredIndex=ind, invertBayes=F) %>%
#       as.data.frame()
# }) %>% bind_cols
#   colnames(results) <- names
#   results
# }
```


### Subchallenge 1

```{r echo=FALSE, message=FALSE}
bayes_top.sc1
```

### Subchallenge 2

```{r echo=FALSE, message=FALSE}
bayes_top.sc2
```

### Subchallenge 3

```{r echo=FALSE, message=FALSE}
bayes_top.sc3
```

---

## Plot Results

Plot boxplot of all scores, coloring the boxes by Bayes factor. 

```{r echo=TRUE, message=FALSE}
plot_results <- function(results, bayes, subchallenge) {
  first_score <- mean(results[,1])
  
  res <- results %>%
    as_tibble() %>%
    gather(submission, bs_score) %>%
    left_join(bayes) %>%
    group_by(submission) %>% 
    mutate(mean_score = mean(bs_score)) %>% 
    ungroup() %>% 
    mutate(bayes_category=case_when(
      bayes == 0 ~ "Reference",
      mean_score > first_score & bayes<=3 ~ "<3, better",
      mean_score > first_score & bayes>3 ~ ">3, better",
      mean_score < first_score & bayes<=3 ~ "<3, worse",
      mean_score < first_score & bayes>3 ~">3, worse")) 
  
  lvls <- colnames(results)
  res$submission <- factor(res$submission, levels = lvls)
  
  labs <- glue::glue("{res$submission}+")
  
  ggplot(res, aes(
      x=submission,
      y=bs_score,
      color=bayes_category
    )) +
    geom_boxplot() +
    theme_bw() +
    scale_color_manual(values = c(
      "Reference"="#000000", 
      '<3, worse' = '#AA3F0E', 
      ">3, worse" = '#EF7B45', 
      '<3, better' = '#024DB6', 
      ">3, better" = '#3689FC'),
      name = "Bayes Factor") +
    labs(x="Team", y=paste("Bootstrapped", subchallenge, "\nSpearman correlation")) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
  }
```

### Subchallenge 1

```{r echo=TRUE, message=FALSE}
plt1 <-plot_results(boot.sc1, bayes_top.sc1, "SC1")
```


### Subchallenge 2


```{r echo=TRUE, message=FALSE}
plt2 <- plot_results(boot.sc2, bayes_top.sc2, "SC2")
```

### Subchallenge 3


```{r echo=TRUE, message=FALSE}
plt3 <- plot_results(boot.sc3, bayes_top.sc3, "SC3")
```


```{r echo=TRUE, fig.height= 7, fig.width= 21}
cowplot::plot_grid(plt1, plt2, plt3, nrow = 1, labels = "AUTO")
ggplot2::ggsave("ra2_ensemble_spearman.pdf", height = 5, width = 15, units = "in")
```
